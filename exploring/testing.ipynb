{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(shape=(23))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Dropbox\\Academics\\Master (Msc)\\MSC AI\\Sem 2\\Machine Learning\\Assignment\\Mini Project\\SubjectSentimentModelling\\basic_exploring\\testing.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Dropbox/Academics/Master%20%28Msc%29/MSC%20AI/Sem%202/Machine%20Learning/Assignment/Mini%20Project/SubjectSentimentModelling/basic_exploring/testing.ipynb#ch0000011?line=0'>1</a>\u001b[0m \u001b[39m0\u001b[39;49m \u001b[39m/\u001b[39;49m \u001b[39m0\u001b[39;49m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "0 / 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32md:\\Dropbox\\Academics\\Master (Msc)\\MSC AI\\Sem 2\\Machine Learning\\Assignment\\Mini Project\\SubjectSentimentModelling\\basic_exploring\\testing.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Dropbox/Academics/Master%20%28Msc%29/MSC%20AI/Sem%202/Machine%20Learning/Assignment/Mini%20Project/SubjectSentimentModelling/basic_exploring/testing.ipynb#ch0000004?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m StratifiedKFold\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3])\n",
    "b = np.array([4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([a, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack([a, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack([a, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[np.array([0,2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'DT'), ('room', 'NN'), ('is', 'VBZ'), ('too', 'RB'), ('noisy', 'JJ')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag((word_tokenize(\"This room is too noisy\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipe_lemmatization(text: str) -> str:\n",
    "    txt = \"\"\n",
    "    for w in word_tokenize(text):\n",
    "        txt += lemmatizer.lemmatize(w) + ' '\n",
    "    return txt.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This room is too noisy'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_lemmatization(\"This room is too noisy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfdata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Dropbox\\Academics\\Master (Msc)\\MSC AI\\Sem 2\\Machine Learning\\Assignment\\Mini Project\\SubjectSentimentModelling\\basic_exploring\\testing.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 236>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Dropbox/Academics/Master%20%28Msc%29/MSC%20AI/Sem%202/Machine%20Learning/Assignment/Mini%20Project/SubjectSentimentModelling/basic_exploring/testing.ipynb#ch0000005?line=229'>230</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m numer \u001b[39m/\u001b[39m denom\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Dropbox/Academics/Master%20%28Msc%29/MSC%20AI/Sem%202/Machine%20Learning/Assignment/Mini%20Project/SubjectSentimentModelling/basic_exploring/testing.ipynb#ch0000005?line=232'>233</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_json(\u001b[39m'\u001b[39m\u001b[39m../data/Amazon_GroceryandGourmetFood.json\u001b[39m\u001b[39m'\u001b[39m, lines\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/Dropbox/Academics/Master%20%28Msc%29/MSC%20AI/Sem%202/Machine%20Learning/Assignment/Mini%20Project/SubjectSentimentModelling/basic_exploring/testing.ipynb#ch0000005?line=235'>236</a>\u001b[0m dfdata\u001b[39m.\u001b[39mfillna(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Dropbox/Academics/Master%20%28Msc%29/MSC%20AI/Sem%202/Machine%20Learning/Assignment/Mini%20Project/SubjectSentimentModelling/basic_exploring/testing.ipynb#ch0000005?line=237'>238</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(dfdata[\u001b[39m'\u001b[39m\u001b[39moverall\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munique()) \u001b[39m!=\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Dropbox/Academics/Master%20%28Msc%29/MSC%20AI/Sem%202/Machine%20Learning/Assignment/Mini%20Project/SubjectSentimentModelling/basic_exploring/testing.ipynb#ch0000005?line=238'>239</a>\u001b[0m     dfdata[\u001b[39m'\u001b[39m\u001b[39moverallOri\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m dfdata[\u001b[39m'\u001b[39m\u001b[39moverall\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dfdata' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# general tools\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import io\n",
    "import re\n",
    "import datetime as dt\n",
    "import json\n",
    "import requests\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import statistics\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# preprocessing & utils\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# ml models\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# others\n",
    "from wordcloud import WordCloud\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# processsing\n",
    "def pipe_cleaningText(text: str) -> str:\n",
    "    txt = text.lower()\n",
    "    return txt\n",
    "\n",
    "def pipe_removeStopWords(text: str) -> str:\n",
    "    return \" \".join([w for w in word_tokenize(text) if w not in stop_words])\n",
    "\n",
    "def pipe_lemmatization(text: str) -> str:\n",
    "    txt = \"\"\n",
    "    for w in word_tokenize(text):\n",
    "        txt += lemmatizer.lemmatize(w) + ' '\n",
    "    return txt.strip()\n",
    "\n",
    "def pipe_addPos(text: str) -> str:\n",
    "    txt = \"\"\n",
    "    for w in pos_tag(word_tokenize(text)):\n",
    "        txt += '__'.join(w) + ' '\n",
    "    return txt.strip()\n",
    "\n",
    "def pipe_normalized(text: str, bin: int) -> str:\n",
    "    txt = \"\"\n",
    "    elements = word_tokenize(text)\n",
    "    n = len(elements)\n",
    "    for i, w in enumerate(elements):\n",
    "        normVal = math.floor((i / (n - 1)) * (bin - 1)) + 1 if n != 1 else 1\n",
    "        txt += w + '__' + str(normVal) + ' '\n",
    "    return txt.strip()\n",
    "\n",
    "def pipe_subjectCleaning(text: str) -> str:\n",
    "    txt = \"\"\n",
    "    for s in sent_tokenize(text):\n",
    "        for w in pos_tag(word_tokenize(s)):\n",
    "            if w[1] != 'NNP':\n",
    "                continue\n",
    "            txt += w[0] + ' '\n",
    "    return txt.strip()\n",
    "\n",
    "# feature extraction\n",
    "def feature_tfidfUni(series: pd.Series, max_features=None) -> (sp.sparse.csr_matrix, TfidfVectorizer):\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,1), max_features=max_features)\n",
    "    tfidf_vectorizer.fit(series)\n",
    "    return tfidf_vectorizer.transform(series), tfidf_vectorizer\n",
    "\n",
    "def feature_bowUni(series: pd.Series, max_features=None) -> (sp.sparse.csr_matrix, CountVectorizer):\n",
    "    bow_vectorizer = CountVectorizer(ngram_range=(1,1), max_features=max_features)\n",
    "    bow_vectorizer.fit(series)\n",
    "    return bow_vectorizer.transform(series), bow_vectorizer\n",
    "\n",
    "def feature_tfidfUniBi(series: pd.Series, max_features=None) -> (sp.sparse.csr_matrix, TfidfVectorizer):\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=max_features)\n",
    "    tfidf_vectorizer.fit(series)\n",
    "    return tfidf_vectorizer.transform(series), tfidf_vectorizer\n",
    "\n",
    "def feature_bowUniBi(series: pd.Series, max_features=None) -> (sp.sparse.csr_matrix, CountVectorizer):\n",
    "    bow_vectorizer = CountVectorizer(ngram_range=(1,2), max_features=max_features)\n",
    "    bow_vectorizer.fit(series)\n",
    "    return bow_vectorizer.transform(series), bow_vectorizer\n",
    "\n",
    "# sampling\n",
    "def balancing_upsampling(data: pd.DataFrame, target: int=None) -> pd.DataFrame:\n",
    "    group_res = data.groupby(by=['overall'])['overall'].count()\n",
    "    highest_n = max(group_res.values) if not target else target\n",
    "    thedata = data[data.overall == group_res[group_res == highest_n].index[0]] if not target else data.drop(data.index)\n",
    "    for i in group_res.index:\n",
    "        if group_res.loc[i] == highest_n:\n",
    "            continue\n",
    "        tdata = data[data.overall == i]\n",
    "        sample_ind = np.random.choice(tdata.index, highest_n - group_res.loc[i], replace=True)\n",
    "        thedata = pd.concat([thedata, tdata, tdata.loc[sample_ind]])\n",
    "    return thedata\n",
    "\n",
    "def balancing_downsampling(data: pd.DataFrame, target: int=None) -> pd.DataFrame:\n",
    "    group_res = data.groupby(by=['overall'])['overall'].count()\n",
    "    lowest_n = min(group_res.values) if not target else target\n",
    "    thedata = data[data.overall == group_res[group_res == lowest_n].index[0]] if not target else data.drop(data.index)\n",
    "    for i in group_res.index:\n",
    "        if group_res.loc[i] == lowest_n:\n",
    "            continue\n",
    "        tdata = data[data.overall == i]\n",
    "        sample_ind = np.random.choice(tdata.index, lowest_n, replace=False)\n",
    "        thedata = pd.concat([thedata, tdata.loc[sample_ind]])\n",
    "    return thedata\n",
    "\n",
    "# testing\n",
    "def accuracyTrainTest(model, trainX: np.array, trainY: np.array, valX: np.array, valY: np.array, testX: np.array, testY: np.array) -> (float, float, float):\n",
    "    mod = model.fit(trainX, trainY)\n",
    "    yfit = mod.predict(trainX)\n",
    "    ypredVal = mod.predict(valX)\n",
    "    ypredTest = mod.predict(testX)\n",
    "    \n",
    "    # train result, validation result, test result\n",
    "    return accuracy_score(trainY, yfit), accuracy_score(valY, ypredVal), accuracy_score(testY, ypredTest)\n",
    "    \n",
    "def loop_testing(model, n_test: int, xArr: sp.sparse.csr_matrix, yArr: np.array, testXArr: sp.sparse.csr_matrix, testYArr: np.array) -> dict[str: list([float, float])]:\n",
    "    thedict = {}\n",
    "\n",
    "    i = 0\n",
    "    n = xArr.shape[0]\n",
    "    n_sample = math.ceil(n / n_test)\n",
    "    x_sam, y_sam = np.array([]), np.array([])\n",
    "    while i < n_test:\n",
    "        percent_sample = n_sample/xArr.shape[0]\n",
    "        if percent_sample >= 1:\n",
    "            xmain, ymain = xArr, yArr\n",
    "        else:\n",
    "            xmain, xArr, ymain, yArr = train_test_split(xArr, yArr, train_size=percent_sample, stratify=yArr, random_state=123)\n",
    "        \n",
    "        x_sam = sp.sparse.vstack((x_sam, xmain)) if x_sam.shape[0] != 0 else xmain\n",
    "        y_sam = np.hstack((y_sam, ymain)) if y_sam.shape[0] != 0 else ymain\n",
    "        xtrain, xtest, ytrain, ytest = train_test_split(x_sam, y_sam, train_size=0.9, stratify=y_sam, random_state=123)\n",
    "\n",
    "        resTrain, resVal, resTest = accuracyTrainTest(model, xtrain, ytrain, xtest, ytest, testXArr, testYArr)\n",
    "        thedict[x_sam.shape[0]] = [resTrain, resVal, resTest]\n",
    "\n",
    "        i += 1\n",
    "    return thedict\n",
    "\n",
    "# visualization\n",
    "def show_result(ypred: np.array, ytarget: np.array) -> None:\n",
    "    label = ['Positive', 'Neutral', 'Negative']\n",
    "    report = classification_report(ypred, ytarget, labels=label)\n",
    "    conf_matrix = confusion_matrix(ypred, ytarget, labels=label)\n",
    "\n",
    "    group_names = ['True Pos', 'False Neu', 'False Neg',\n",
    "                   'False Pos', 'True Neu', 'False Neg',\n",
    "                   'False Pos', 'False Neu', 'True Neg']\n",
    "    group_percentages = ['{0:.2%}'.format(value) for value in conf_matrix.flatten() / np.sum(conf_matrix)]\n",
    "    labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(group_names, group_percentages, conf_matrix.flatten())]\n",
    "    labels = np.asarray(labels).reshape(3, 3)\n",
    "\n",
    "    df_cm = pd.DataFrame(data=conf_matrix, index=label, columns=label)\n",
    "\n",
    "    print(report)\n",
    "    fix, axe = plt.subplots(figsize=(15, 8), nrows=1, ncols=1)\n",
    "    sns.heatmap(df_cm, xticklabels=label, yticklabels=label, annot=labels, ax=axe, cmap = 'Blues', fmt = '')\n",
    "    plt.show();\n",
    "\n",
    "def visualized_loopTesting(result_loopTesting: dict[str: list([float, float])]) -> None:\n",
    "    train_res, val_res, test_res = dict(), dict(), dict()\n",
    "    for res in result_loopTesting:\n",
    "        train_res[res] = result[res][0]\n",
    "        val_res[res] = result[res][1]\n",
    "        test_res[res] = result[res][2]\n",
    "\n",
    "    fig, axe = plt.subplots(figsize=(15, 8), nrows=1, ncols=1)\n",
    "    sns.lineplot(y=train_res.values(), x=train_res.keys() , ax=axe, label='train-result')\n",
    "    sns.lineplot(y=val_res.values(), x=val_res.keys(), ax=axe, label='val-result')\n",
    "    sns.lineplot(y=test_res.values(), x=test_res.keys(), ax=axe, label='test-result')\n",
    "    axe.set_title(\"Highest Score: {}\".format(max(test_res.values())))\n",
    "    axe.set_xlabel(\"Dataset Size\")\n",
    "    axe.set_ylabel(\"Accuracy\")\n",
    "    axe.set_ylim([0.65, 1.0])\n",
    "    axe.xaxis.set_major_formatter(FormatStrFormatter('%.0f'))\n",
    "    plt.show();\n",
    "\n",
    "# others\n",
    "def correlation(x: list, y: list) -> float:\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    bar_x = x.mean()\n",
    "    bar_y = y.mean()\n",
    "    sum_xixbar_yiybar = sum((x - bar_x) * (y - bar_y))\n",
    "    sum_xixbar2 = sum((x - bar_x) ** 2)\n",
    "    sum_yiybar2 = sum((y - bar_y) ** 2)\n",
    "\n",
    "    numer = sum_xixbar_yiybar\n",
    "    denom = (sum_xixbar2 * sum_yiybar2) ** (1 / 2)\n",
    "\n",
    "    r = numer / denom if denom != 0 else 0\n",
    "\n",
    "    return r\n",
    "\n",
    "def gradient(x: list, y: list) -> float:\n",
    "    p1 = [x[0], y[0]]\n",
    "    p2 = [x[2], y[2]]\n",
    "\n",
    "    numer = p1[1] - p2[1]\n",
    "    denom = p1[0] - p2[0]\n",
    "\n",
    "    return numer / denom\n",
    "    \n",
    "\n",
    "df = pd.read_json('../data/Amazon_GroceryandGourmetFood.json', lines=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdata = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdata.fillna(\"\", inplace=True)\n",
    "\n",
    "if len(dfdata['overall'].unique()) != 3:\n",
    "    dfdata['overallOri'] = dfdata['overall']\n",
    "dfdata['overall'] = dfdata['overallOri'].apply(lambda x: \"Positive\" if x > 3 else \"Negative\" if x < 3 else \"Neutral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_n = 500 * 1000\n",
    "dfdataMain, _ = train_test_split(dfdata, train_size=target_n / dfdata.shape[0], stratify=dfdata['overall'], random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for training & testing\n",
    "target_n = 450 * 1000\n",
    "dfdataTrain, dfdataTest = train_test_split(dfdataMain, train_size=target_n / dfdataMain.shape[0], stratify=dfdataMain['overall'], random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for validation\n",
    "target_n = 45 * 1000\n",
    "dfdataValid, dfdataTrainSub = train_test_split(dfdataTrain, train_size=target_n / dfdataTrain.shape[0], stratify=dfdataTrain['overall'], random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for validation training & testing\n",
    "target_n = 40 * 1000\n",
    "dfdataValidTrain, dfdataValidTest = train_test_split(dfdataValid, train_size=target_n / dfdataValid.shape[0], stratify=dfdataValid['overall'], random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsampling minority group for validation training\n",
    "if statistics.variance(dfdataValidTrain.groupby(by=['overall'])['overall'].count().values) >= 10 * 1000:\n",
    "    dfdataValidTrain = balancing_upsampling(dfdataValidTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(txt: str) -> str:\n",
    "    txt = pipe_cleaningText(txt)\n",
    "    txt = pipe_lemmatization(txt)\n",
    "    txt = pipe_normalized(txt, 5)\n",
    "    return txt\n",
    "    \n",
    "tdf1 = dfdataValidTrain.copy()\n",
    "tdf2 = dfdataValidTest.copy()\n",
    "tdf1['reviewText'] = tdf1['reviewText'].apply(lambda x: pipeline(x))\n",
    "tdf2['reviewText'] = tdf2['reviewText'].apply(lambda x: pipeline(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "xArr, featureModel = feature_bowUniBi(tdf1['reviewText'], 150000)\n",
    "yArr = tdf1['overall'].to_numpy()\n",
    "testXArr, testYArr = featureModel.transform(tdf2['reviewText']), tdf2['overall'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 4: mlp\n",
    "\n",
    "mlp_model = MLPClassifier()\n",
    "result = loop_testing(mlp_model, 10, xArr, yArr, testXArr, testYArr)\n",
    "print(\"Correlation last 3: {:.4f}\".format(correlation(list(result.keys())[-3:], [x[2] for x in result.values()][-3:])))\n",
    "print(\"Gradient last 3: {:.4f}\".format(gradient(range(1, 4), [x[2] for x in result.values()][-3:])))\n",
    "visualized_loopTesting(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Dropbox\\Academics\\Master (Msc)\\MSC AI\\Sem 2\\Machine Learning\\Assignment\\Mini Project\\SubjectSentimentModelling\\env\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3398: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/Amazon_Electronics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>style</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>vote</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>07 17, 2002</td>\n",
       "      <td>A1N070NS9CJQ2I</td>\n",
       "      <td>0060009810</td>\n",
       "      <td>{'Format:': ' Hardcover'}</td>\n",
       "      <td>Teri Adams</td>\n",
       "      <td>This was the first time I read Garcia-Aguilera...</td>\n",
       "      <td>Hit The Spot!</td>\n",
       "      <td>1026864000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>07 6, 2002</td>\n",
       "      <td>A3P0KRKOBQK1KN</td>\n",
       "      <td>0060009810</td>\n",
       "      <td>{'Format:': ' Hardcover'}</td>\n",
       "      <td>Willa C.</td>\n",
       "      <td>As with all of Ms. Garcia-Aguilera's books, I ...</td>\n",
       "      <td>one hot summer is HOT HOT HOT!</td>\n",
       "      <td>1025913600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>07 3, 2002</td>\n",
       "      <td>A192HO2ICJ75VU</td>\n",
       "      <td>0060009810</td>\n",
       "      <td>{'Format:': ' Hardcover'}</td>\n",
       "      <td>Kit</td>\n",
       "      <td>I've not read any of Ms Aguilera's works befor...</td>\n",
       "      <td>One Hot Summer</td>\n",
       "      <td>1025654400</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>06 30, 2002</td>\n",
       "      <td>A2T278FKFL3BLT</td>\n",
       "      <td>0060009810</td>\n",
       "      <td>{'Format:': ' Hardcover'}</td>\n",
       "      <td>Andres</td>\n",
       "      <td>This romance novel is right up there with the ...</td>\n",
       "      <td>I love this book!</td>\n",
       "      <td>1025395200</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>06 28, 2002</td>\n",
       "      <td>A2ZUXVTW8RXBXW</td>\n",
       "      <td>0060009810</td>\n",
       "      <td>{'Format:': ' Hardcover'}</td>\n",
       "      <td>John</td>\n",
       "      <td>Carolina Garcia Aguilera has done it again.  S...</td>\n",
       "      <td>One Hot Book</td>\n",
       "      <td>1025222400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall  verified   reviewTime      reviewerID        asin  \\\n",
       "0        5      True  07 17, 2002  A1N070NS9CJQ2I  0060009810   \n",
       "1        5     False   07 6, 2002  A3P0KRKOBQK1KN  0060009810   \n",
       "2        5     False   07 3, 2002  A192HO2ICJ75VU  0060009810   \n",
       "3        4     False  06 30, 2002  A2T278FKFL3BLT  0060009810   \n",
       "4        5     False  06 28, 2002  A2ZUXVTW8RXBXW  0060009810   \n",
       "\n",
       "                       style reviewerName  \\\n",
       "0  {'Format:': ' Hardcover'}   Teri Adams   \n",
       "1  {'Format:': ' Hardcover'}     Willa C.   \n",
       "2  {'Format:': ' Hardcover'}          Kit   \n",
       "3  {'Format:': ' Hardcover'}       Andres   \n",
       "4  {'Format:': ' Hardcover'}         John   \n",
       "\n",
       "                                          reviewText  \\\n",
       "0  This was the first time I read Garcia-Aguilera...   \n",
       "1  As with all of Ms. Garcia-Aguilera's books, I ...   \n",
       "2  I've not read any of Ms Aguilera's works befor...   \n",
       "3  This romance novel is right up there with the ...   \n",
       "4  Carolina Garcia Aguilera has done it again.  S...   \n",
       "\n",
       "                          summary  unixReviewTime vote image  \n",
       "0                   Hit The Spot!      1026864000  NaN   NaN  \n",
       "1  one hot summer is HOT HOT HOT!      1025913600  NaN   NaN  \n",
       "2                  One Hot Summer      1025654400    2   NaN  \n",
       "3               I love this book!      1025395200    3   NaN  \n",
       "4                    One Hot Book      1025222400  NaN   NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20941539"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "overall\n",
       "1     2409983\n",
       "2     1136973\n",
       "3     1526548\n",
       "4     3298596\n",
       "5    12569439\n",
       "Name: overall, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(by=['overall']).overall.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "overall\n",
       "1    1.0\n",
       "2    2.0\n",
       "3    3.0\n",
       "4    4.0\n",
       "5    5.0\n",
       "Name: overall, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(by=['overall']).overall.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>style</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>vote</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20941534</th>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>04 29, 2017</td>\n",
       "      <td>A1RH0C7YHWZQER</td>\n",
       "      <td>B01HJF704M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Randy Garmire</td>\n",
       "      <td>Had it 1 day and it quit working, will be retu...</td>\n",
       "      <td>JUNK</td>\n",
       "      <td>1493424000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20941535</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>04 17, 2017</td>\n",
       "      <td>A2955VBJEJZ4S</td>\n",
       "      <td>B01HJF704M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BJN</td>\n",
       "      <td>Received item in 2 days. Product worked as adv...</td>\n",
       "      <td>Instructions easy and clear</td>\n",
       "      <td>1492387200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20941536</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>03 31, 2017</td>\n",
       "      <td>A1FGCIRPRNZWD5</td>\n",
       "      <td>B01HJF704M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brando</td>\n",
       "      <td>I have it plugged into a usb extension on my g...</td>\n",
       "      <td>Works well enough..</td>\n",
       "      <td>1490918400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20941537</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>02 16, 2017</td>\n",
       "      <td>AOEG7L8HI8DXJ</td>\n",
       "      <td>B01HJF704M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mack</td>\n",
       "      <td>Fast delivery product was simple to use</td>\n",
       "      <td>Good product</td>\n",
       "      <td>1487203200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20941538</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>02 11, 2017</td>\n",
       "      <td>A1BXCUK9AT7KWG</td>\n",
       "      <td>B01HJF704M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Clancy</td>\n",
       "      <td>Working as advertised, so far no problems.</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1486771200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          overall  verified   reviewTime      reviewerID        asin style  \\\n",
       "20941534        1      True  04 29, 2017  A1RH0C7YHWZQER  B01HJF704M   NaN   \n",
       "20941535        5      True  04 17, 2017   A2955VBJEJZ4S  B01HJF704M   NaN   \n",
       "20941536        5      True  03 31, 2017  A1FGCIRPRNZWD5  B01HJF704M   NaN   \n",
       "20941537        5      True  02 16, 2017   AOEG7L8HI8DXJ  B01HJF704M   NaN   \n",
       "20941538        5      True  02 11, 2017  A1BXCUK9AT7KWG  B01HJF704M   NaN   \n",
       "\n",
       "           reviewerName                                         reviewText  \\\n",
       "20941534  Randy Garmire  Had it 1 day and it quit working, will be retu...   \n",
       "20941535            BJN  Received item in 2 days. Product worked as adv...   \n",
       "20941536         Brando  I have it plugged into a usb extension on my g...   \n",
       "20941537           Mack            Fast delivery product was simple to use   \n",
       "20941538         Clancy         Working as advertised, so far no problems.   \n",
       "\n",
       "                              summary  unixReviewTime vote image  \n",
       "20941534                         JUNK      1493424000  NaN   NaN  \n",
       "20941535  Instructions easy and clear      1492387200  NaN   NaN  \n",
       "20941536          Works well enough..      1490918400  NaN   NaN  \n",
       "20941537                 Good product      1487203200  NaN   NaN  \n",
       "20941538                   Five Stars      1486771200  NaN   NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "overall                  0\n",
       "verified                 0\n",
       "reviewTime               0\n",
       "reviewerID               0\n",
       "asin                     0\n",
       "style             10476370\n",
       "reviewerName          2668\n",
       "reviewText            9670\n",
       "summary               4896\n",
       "unixReviewTime           0\n",
       "vote              18254610\n",
       "image             20594419\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           139\n",
       "1            42\n",
       "2            51\n",
       "3           105\n",
       "4            45\n",
       "           ... \n",
       "20941534     11\n",
       "20941535     13\n",
       "20941536     75\n",
       "20941537      7\n",
       "20941538      7\n",
       "Name: reviewText, Length: 20931869, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[~df.reviewText.isnull()]['reviewText'].apply(lambda x: len(x.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "46bc6b800f911b1c1d0f43a06dbc3139270a12e6ebc6e060d97d05b2a2df1825"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
