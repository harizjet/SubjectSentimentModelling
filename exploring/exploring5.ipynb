{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general tools\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import io\n",
    "import re\n",
    "import datetime as dt\n",
    "import json\n",
    "import requests\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import statistics\n",
    "import os\n",
    "from typing import Tuple, Dict, List\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# preprocessing & utils\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# ml models\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# others\n",
    "from wordcloud import WordCloud\n",
    "from matplotlib.ticker import FormatStrFormatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h3>Constant</h3>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h3>Utility Function</h3>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processsing\n",
    "def pipe_cleaningText(text: str) -> str:\n",
    "    txt = text.lower()\n",
    "    return txt\n",
    "\n",
    "def pipe_removeStopWords(text: str, tokenized: bool = False) -> str:\n",
    "    if tokenized:\n",
    "        return \" \".join([w for w in text.split(\" \") if w not in stop_words])\n",
    "    return \" \".join([w for w in word_tokenize(text) if w not in stop_words])\n",
    "\n",
    "def pipe_lemmatization(text: str, tokenized: bool = False) -> str:\n",
    "    txt = \"\"\n",
    "    collections = word_tokenize(text) if not tokenized else text.split(\" \")\n",
    "    for w in collections:\n",
    "        txt += lemmatizer.lemmatize(w) + ' '\n",
    "    return txt.strip()\n",
    "\n",
    "def pipe_addPos(text: str, tokenized: bool = False) -> str:\n",
    "    txt = \"\"\n",
    "    collections = word_tokenize(text) if not tokenized else text.split(\" \")\n",
    "    for w in pos_tag(collections):\n",
    "        txt += '__'.join(w) + ' '\n",
    "    return txt.strip()\n",
    "\n",
    "def pipe_normalized(text: str, bin: int, tokenized: bool = False) -> str:\n",
    "    txt = \"\"\n",
    "    collections = word_tokenize(text) if not tokenized else text.split(\" \")\n",
    "    n = len(collections)\n",
    "    for i, w in enumerate(collections):\n",
    "        normVal = math.floor((i / (n - 1)) * (bin - 1)) + 1 if n != 1 else 1\n",
    "        txt += w + '__' + str(normVal) + ' '\n",
    "    return txt.strip()\n",
    "\n",
    "def pipe_subjectCleaning(text: str, tokenized: bool = False) -> str:\n",
    "    txt = \"\"\n",
    "    collections = sent_tokenize(text) if not tokenized else text.split(\" \")\n",
    "    for s in collections:\n",
    "        for w in pos_tag(word_tokenize(s)):\n",
    "            if w[1] != 'NNP':\n",
    "                continue\n",
    "            txt += w[0] + ' '\n",
    "    return txt.strip()\n",
    "\n",
    "# feature extraction\n",
    "def feature_tfidfUni(series: pd.Series, max_features=None) -> Tuple[sp.sparse.csr_matrix, TfidfVectorizer]:\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,1), max_features=max_features)\n",
    "    tfidf_vectorizer.fit(series)\n",
    "    return tfidf_vectorizer.transform(series), tfidf_vectorizer\n",
    "\n",
    "def feature_bowUni(series: pd.Series, max_features=None) -> Tuple[sp.sparse.csr_matrix, CountVectorizer]:\n",
    "    bow_vectorizer = CountVectorizer(ngram_range=(1,1), max_features=max_features)\n",
    "    bow_vectorizer.fit(series)\n",
    "    return bow_vectorizer.transform(series), bow_vectorizer\n",
    "\n",
    "def feature_tfidfUniBi(series: pd.Series, max_features=None) -> Tuple[sp.sparse.csr_matrix, TfidfVectorizer]:\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=max_features)\n",
    "    tfidf_vectorizer.fit(series)\n",
    "    return tfidf_vectorizer.transform(series), tfidf_vectorizer\n",
    "\n",
    "def feature_bowUniBi(series: pd.Series, max_features=None) -> Tuple[sp.sparse.csr_matrix, CountVectorizer]:\n",
    "    bow_vectorizer = CountVectorizer(ngram_range=(1,2), max_features=max_features)\n",
    "    bow_vectorizer.fit(series)\n",
    "    return bow_vectorizer.transform(series), bow_vectorizer\n",
    "\n",
    "# sampling\n",
    "def balancing_upsampling(data: pd.DataFrame, target: int=None) -> pd.DataFrame:\n",
    "    group_res = data.groupby(by=['overall'])['overall'].count()\n",
    "    highest_n = max(group_res.values) if not target else target\n",
    "    thedata = data[data.overall == group_res[group_res == highest_n].index[0]] if not target else data.drop(data.index)\n",
    "    for i in group_res.index:\n",
    "        if group_res.loc[i] == highest_n:\n",
    "            continue\n",
    "        tdata = data[data.overall == i]\n",
    "        sample_ind = np.random.choice(tdata.index, highest_n - group_res.loc[i], replace=True)\n",
    "        thedata = pd.concat([thedata, tdata, tdata.loc[sample_ind]])\n",
    "    return thedata\n",
    "\n",
    "def balancing_downsampling(data: pd.DataFrame, target: int=None) -> pd.DataFrame:\n",
    "    group_res = data.groupby(by=['overall'])['overall'].count()\n",
    "    lowest_n = min(group_res.values) if not target else target\n",
    "    thedata = data[data.overall == group_res[group_res == lowest_n].index[0]] if not target else data.drop(data.index)\n",
    "    for i in group_res.index:\n",
    "        if group_res.loc[i] == lowest_n:\n",
    "            continue\n",
    "        tdata = data[data.overall == i]\n",
    "        sample_ind = np.random.choice(tdata.index, lowest_n, replace=False)\n",
    "        thedata = pd.concat([thedata, tdata.loc[sample_ind]])\n",
    "    return thedata\n",
    "\n",
    "# testing\n",
    "def accuracyTrainTest(model, trainX: np.array, trainY: np.array, valX: np.array, valY: np.array, testX: np.array, testY: np.array) -> Tuple[float, float, float]:\n",
    "    mod = model.fit(trainX, trainY)\n",
    "    yfit = mod.predict(trainX)\n",
    "    ypredVal = mod.predict(valX)\n",
    "    ypredTest = mod.predict(testX)\n",
    "    \n",
    "    # train result, validation result, test result\n",
    "    return accuracy_score(trainY, yfit), accuracy_score(valY, ypredVal), accuracy_score(testY, ypredTest)\n",
    "    \n",
    "def loop_testing(model, n_test: int, xArr: sp.sparse.csr_matrix, yArr: np.array, testXArr: sp.sparse.csr_matrix, testYArr: np.array) -> Dict[str, List[float]]:\n",
    "    thedict = {}\n",
    "\n",
    "    i = 0\n",
    "    n = xArr.shape[0]\n",
    "    n_sample = math.ceil(n / n_test)\n",
    "    x_sam, y_sam = np.array([]), np.array([])\n",
    "    while i < n_test:\n",
    "        percent_sample = n_sample/xArr.shape[0]\n",
    "        if percent_sample >= 1:\n",
    "            xmain, ymain = xArr, yArr\n",
    "        else:\n",
    "            xmain, xArr, ymain, yArr = train_test_split(xArr, yArr, train_size=percent_sample, stratify=yArr, random_state=123)\n",
    "        \n",
    "        x_sam = sp.sparse.vstack((x_sam, xmain)) if x_sam.shape[0] != 0 else xmain\n",
    "        y_sam = np.hstack((y_sam, ymain)) if y_sam.shape[0] != 0 else ymain\n",
    "        xtrain, xtest, ytrain, ytest = train_test_split(x_sam, y_sam, train_size=0.9, stratify=y_sam, random_state=123)\n",
    "\n",
    "        resTrain, resVal, resTest = accuracyTrainTest(model, xtrain, ytrain, xtest, ytest, testXArr, testYArr)\n",
    "        thedict[x_sam.shape[0]] = [resTrain, resVal, resTest]\n",
    "\n",
    "        i += 1\n",
    "    return thedict\n",
    "\n",
    "# visualization\n",
    "def show_result(ypred: np.array, ytarget: np.array) -> None:\n",
    "    label = ['Positive', 'Neutral', 'Negative']\n",
    "    report = classification_report(ypred, ytarget, labels=label)\n",
    "    conf_matrix = confusion_matrix(ypred, ytarget, labels=label)\n",
    "\n",
    "    group_names = ['True Pos', 'False Neu', 'False Neg',\n",
    "                   'False Pos', 'True Neu', 'False Neg',\n",
    "                   'False Pos', 'False Neu', 'True Neg']\n",
    "    group_percentages = ['{0:.2%}'.format(value) for value in conf_matrix.flatten() / np.sum(conf_matrix)]\n",
    "    labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(group_names, group_percentages, conf_matrix.flatten())]\n",
    "    labels = np.asarray(labels).reshape(3, 3)\n",
    "\n",
    "    df_cm = pd.DataFrame(data=conf_matrix, index=label, columns=label)\n",
    "\n",
    "    print(report)\n",
    "    fix, axe = plt.subplots(figsize=(15, 8), nrows=1, ncols=1)\n",
    "    sns.heatmap(df_cm, xticklabels=label, yticklabels=label, annot=labels, ax=axe, cmap = 'Blues', fmt = '')\n",
    "    plt.show();\n",
    "\n",
    "# visualization\n",
    "def show_result2group(ypred: np.array, ytarget: np.array) -> None:\n",
    "    label = ['Positive', 'Negative']\n",
    "    report = classification_report(ypred, ytarget, labels=label)\n",
    "    conf_matrix = confusion_matrix(ypred, ytarget, labels=label)\n",
    "\n",
    "    group_names = ['True Pos', 'False Neg',\n",
    "                   'False Pos', 'True Neg']\n",
    "    group_percentages = ['{0:.2%}'.format(value) for value in conf_matrix.flatten() / np.sum(conf_matrix)]\n",
    "    labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(group_names, group_percentages, conf_matrix.flatten())]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "    df_cm = pd.DataFrame(data=conf_matrix, index=label, columns=label)\n",
    "\n",
    "    print(report)\n",
    "    fix, axe = plt.subplots(figsize=(15, 8), nrows=1, ncols=1)\n",
    "    sns.heatmap(df_cm, xticklabels=label, yticklabels=label, annot=labels, ax=axe, cmap = 'Blues', fmt = '')\n",
    "    plt.show();\n",
    "\n",
    "def show_result5group(ypred: np.array, ytarget: np.array) -> None:\n",
    "    label = [1,2,3,4,5][::-1]\n",
    "    report = classification_report(ypred, ytarget, labels=label)\n",
    "    conf_matrix = confusion_matrix(ypred, ytarget, labels=label)\n",
    "\n",
    "    group_names = ['True Pos', 'False Meh-Pos', 'False Neu', 'False Meh-Neg', 'False Neg',\n",
    "                   'False Pos', 'True Meh-Pos', 'False Neu', 'False Meh-Neg', 'False Neg',\n",
    "                   'False Pos', 'False Meh-Pos', 'True Neu', 'False Meh-Neg', 'False Neg',\n",
    "                   'False Pos', 'False Meh-Pos', 'False Neu', 'True Meh-Neg', 'False Neg',\n",
    "                   'False Pos', 'False Meh-Pos', 'False Neu', 'False Meh-Neg', 'True Neg']\n",
    "    group_percentages = ['{0:.2%}'.format(value) for value in conf_matrix.flatten() / np.sum(conf_matrix)]\n",
    "    labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(group_names, group_percentages, conf_matrix.flatten())]\n",
    "    labels = np.asarray(labels).reshape(5, 5)\n",
    "\n",
    "    df_cm = pd.DataFrame(data=conf_matrix, index=label, columns=label)\n",
    "\n",
    "    print(report)\n",
    "    fix, axe = plt.subplots(figsize=(15, 8), nrows=1, ncols=1)\n",
    "    sns.heatmap(df_cm, xticklabels=label, yticklabels=label, annot=labels, ax=axe, cmap = 'Blues', fmt = '')\n",
    "    plt.show();\n",
    "\n",
    "def visualized_loopTesting(result_loopTesting: Dict[str, List[float]]) -> None:\n",
    "    train_res, val_res, test_res = dict(), dict(), dict()\n",
    "    for res in result_loopTesting:\n",
    "        train_res[res] = result[res][0]\n",
    "        val_res[res] = result[res][1]\n",
    "        test_res[res] = result[res][2]\n",
    "\n",
    "    fig, axe = plt.subplots(figsize=(15, 8), nrows=1, ncols=1)\n",
    "    sns.lineplot(y=train_res.values(), x=train_res.keys() , ax=axe, label='train-result')\n",
    "    sns.lineplot(y=val_res.values(), x=val_res.keys(), ax=axe, label='val-result')\n",
    "    sns.lineplot(y=test_res.values(), x=test_res.keys(), ax=axe, label='test-result')\n",
    "    axe.set_title(\"Highest Score: {}\".format(max(test_res.values())))\n",
    "    axe.set_xlabel(\"Dataset Size\")\n",
    "    axe.set_ylabel(\"Accuracy\")\n",
    "    axe.set_ylim([0.65, 1.0])\n",
    "    axe.xaxis.set_major_formatter(FormatStrFormatter('%.0f'))\n",
    "    plt.show();\n",
    "\n",
    "# others\n",
    "def correlation(x: list, y: list) -> float:\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    bar_x = x.mean()\n",
    "    bar_y = y.mean()\n",
    "    sum_xixbar_yiybar = sum((x - bar_x) * (y - bar_y))\n",
    "    sum_xixbar2 = sum((x - bar_x) ** 2)\n",
    "    sum_yiybar2 = sum((y - bar_y) ** 2)\n",
    "\n",
    "    numer = sum_xixbar_yiybar\n",
    "    denom = (sum_xixbar2 * sum_yiybar2) ** (1 / 2)\n",
    "\n",
    "    r = numer / denom if denom != 0 else 0\n",
    "\n",
    "    return r\n",
    "\n",
    "def gradient(x: list, y: list) -> float:\n",
    "    p1 = [x[0], y[0]]\n",
    "    p2 = [x[2], y[2]]\n",
    "\n",
    "    numer = p1[1] - p2[1]\n",
    "    denom = p1[0] - p2[0]\n",
    "\n",
    "    return numer / denom\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h3>Process</h3>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goals\n",
    "- Try more extensive features (CBOW & Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "055ed1be4b0fa29866f2e2aeab85b185190bfa779d52a08d11b266bfb238964b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
