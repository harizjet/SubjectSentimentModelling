{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(shape=(23))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Dropbox\\Academics\\Master (Msc)\\MSC AI\\Sem 2\\Machine Learning\\Assignment\\Mini Project\\SubjectSentimentModelling\\basic_exploring\\testing.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Dropbox/Academics/Master%20%28Msc%29/MSC%20AI/Sem%202/Machine%20Learning/Assignment/Mini%20Project/SubjectSentimentModelling/basic_exploring/testing.ipynb#ch0000011?line=0'>1</a>\u001b[0m \u001b[39m0\u001b[39;49m \u001b[39m/\u001b[39;49m \u001b[39m0\u001b[39;49m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "0 / 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32md:\\Dropbox\\Academics\\Master (Msc)\\MSC AI\\Sem 2\\Machine Learning\\Assignment\\Mini Project\\SubjectSentimentModelling\\basic_exploring\\testing.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Dropbox/Academics/Master%20%28Msc%29/MSC%20AI/Sem%202/Machine%20Learning/Assignment/Mini%20Project/SubjectSentimentModelling/basic_exploring/testing.ipynb#ch0000004?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m StratifiedKFold\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3])\n",
    "b = np.array([4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([a, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack([a, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack([a, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[np.array([0,2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'DT'), ('room', 'NN'), ('is', 'VBZ'), ('too', 'RB'), ('noisy', 'JJ')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag((word_tokenize(\"This room is too noisy\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipe_lemmatization(text: str) -> str:\n",
    "    txt = \"\"\n",
    "    for w in word_tokenize(text):\n",
    "        txt += lemmatizer.lemmatize(w) + ' '\n",
    "    return txt.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This room is too noisy'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_lemmatization(\"This room is too noisy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfdata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Dropbox\\Academics\\Master (Msc)\\MSC AI\\Sem 2\\Machine Learning\\Assignment\\Mini Project\\SubjectSentimentModelling\\basic_exploring\\testing.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 236>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Dropbox/Academics/Master%20%28Msc%29/MSC%20AI/Sem%202/Machine%20Learning/Assignment/Mini%20Project/SubjectSentimentModelling/basic_exploring/testing.ipynb#ch0000005?line=229'>230</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m numer \u001b[39m/\u001b[39m denom\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Dropbox/Academics/Master%20%28Msc%29/MSC%20AI/Sem%202/Machine%20Learning/Assignment/Mini%20Project/SubjectSentimentModelling/basic_exploring/testing.ipynb#ch0000005?line=232'>233</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_json(\u001b[39m'\u001b[39m\u001b[39m../data/Amazon_GroceryandGourmetFood.json\u001b[39m\u001b[39m'\u001b[39m, lines\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/Dropbox/Academics/Master%20%28Msc%29/MSC%20AI/Sem%202/Machine%20Learning/Assignment/Mini%20Project/SubjectSentimentModelling/basic_exploring/testing.ipynb#ch0000005?line=235'>236</a>\u001b[0m dfdata\u001b[39m.\u001b[39mfillna(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Dropbox/Academics/Master%20%28Msc%29/MSC%20AI/Sem%202/Machine%20Learning/Assignment/Mini%20Project/SubjectSentimentModelling/basic_exploring/testing.ipynb#ch0000005?line=237'>238</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(dfdata[\u001b[39m'\u001b[39m\u001b[39moverall\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munique()) \u001b[39m!=\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Dropbox/Academics/Master%20%28Msc%29/MSC%20AI/Sem%202/Machine%20Learning/Assignment/Mini%20Project/SubjectSentimentModelling/basic_exploring/testing.ipynb#ch0000005?line=238'>239</a>\u001b[0m     dfdata[\u001b[39m'\u001b[39m\u001b[39moverallOri\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m dfdata[\u001b[39m'\u001b[39m\u001b[39moverall\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dfdata' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# general tools\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import io\n",
    "import re\n",
    "import datetime as dt\n",
    "import json\n",
    "import requests\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import statistics\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# preprocessing & utils\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# ml models\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# others\n",
    "from wordcloud import WordCloud\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# processsing\n",
    "def pipe_cleaningText(text: str) -> str:\n",
    "    txt = text.lower()\n",
    "    return txt\n",
    "\n",
    "def pipe_removeStopWords(text: str) -> str:\n",
    "    return \" \".join([w for w in word_tokenize(text) if w not in stop_words])\n",
    "\n",
    "def pipe_lemmatization(text: str) -> str:\n",
    "    txt = \"\"\n",
    "    for w in word_tokenize(text):\n",
    "        txt += lemmatizer.lemmatize(w) + ' '\n",
    "    return txt.strip()\n",
    "\n",
    "def pipe_addPos(text: str) -> str:\n",
    "    txt = \"\"\n",
    "    for w in pos_tag(word_tokenize(text)):\n",
    "        txt += '__'.join(w) + ' '\n",
    "    return txt.strip()\n",
    "\n",
    "def pipe_normalized(text: str, bin: int) -> str:\n",
    "    txt = \"\"\n",
    "    elements = word_tokenize(text)\n",
    "    n = len(elements)\n",
    "    for i, w in enumerate(elements):\n",
    "        normVal = math.floor((i / (n - 1)) * (bin - 1)) + 1 if n != 1 else 1\n",
    "        txt += w + '__' + str(normVal) + ' '\n",
    "    return txt.strip()\n",
    "\n",
    "def pipe_subjectCleaning(text: str) -> str:\n",
    "    txt = \"\"\n",
    "    for s in sent_tokenize(text):\n",
    "        for w in pos_tag(word_tokenize(s)):\n",
    "            if w[1] != 'NNP':\n",
    "                continue\n",
    "            txt += w[0] + ' '\n",
    "    return txt.strip()\n",
    "\n",
    "# feature extraction\n",
    "def feature_tfidfUni(series: pd.Series, max_features=None) -> (sp.sparse.csr_matrix, TfidfVectorizer):\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,1), max_features=max_features)\n",
    "    tfidf_vectorizer.fit(series)\n",
    "    return tfidf_vectorizer.transform(series), tfidf_vectorizer\n",
    "\n",
    "def feature_bowUni(series: pd.Series, max_features=None) -> (sp.sparse.csr_matrix, CountVectorizer):\n",
    "    bow_vectorizer = CountVectorizer(ngram_range=(1,1), max_features=max_features)\n",
    "    bow_vectorizer.fit(series)\n",
    "    return bow_vectorizer.transform(series), bow_vectorizer\n",
    "\n",
    "def feature_tfidfUniBi(series: pd.Series, max_features=None) -> (sp.sparse.csr_matrix, TfidfVectorizer):\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=max_features)\n",
    "    tfidf_vectorizer.fit(series)\n",
    "    return tfidf_vectorizer.transform(series), tfidf_vectorizer\n",
    "\n",
    "def feature_bowUniBi(series: pd.Series, max_features=None) -> (sp.sparse.csr_matrix, CountVectorizer):\n",
    "    bow_vectorizer = CountVectorizer(ngram_range=(1,2), max_features=max_features)\n",
    "    bow_vectorizer.fit(series)\n",
    "    return bow_vectorizer.transform(series), bow_vectorizer\n",
    "\n",
    "# sampling\n",
    "def balancing_upsampling(data: pd.DataFrame, target: int=None) -> pd.DataFrame:\n",
    "    group_res = data.groupby(by=['overall'])['overall'].count()\n",
    "    highest_n = max(group_res.values) if not target else target\n",
    "    thedata = data[data.overall == group_res[group_res == highest_n].index[0]] if not target else data.drop(data.index)\n",
    "    for i in group_res.index:\n",
    "        if group_res.loc[i] == highest_n:\n",
    "            continue\n",
    "        tdata = data[data.overall == i]\n",
    "        sample_ind = np.random.choice(tdata.index, highest_n - group_res.loc[i], replace=True)\n",
    "        thedata = pd.concat([thedata, tdata, tdata.loc[sample_ind]])\n",
    "    return thedata\n",
    "\n",
    "def balancing_downsampling(data: pd.DataFrame, target: int=None) -> pd.DataFrame:\n",
    "    group_res = data.groupby(by=['overall'])['overall'].count()\n",
    "    lowest_n = min(group_res.values) if not target else target\n",
    "    thedata = data[data.overall == group_res[group_res == lowest_n].index[0]] if not target else data.drop(data.index)\n",
    "    for i in group_res.index:\n",
    "        if group_res.loc[i] == lowest_n:\n",
    "            continue\n",
    "        tdata = data[data.overall == i]\n",
    "        sample_ind = np.random.choice(tdata.index, lowest_n, replace=False)\n",
    "        thedata = pd.concat([thedata, tdata.loc[sample_ind]])\n",
    "    return thedata\n",
    "\n",
    "# testing\n",
    "def accuracyTrainTest(model, trainX: np.array, trainY: np.array, valX: np.array, valY: np.array, testX: np.array, testY: np.array) -> (float, float, float):\n",
    "    mod = model.fit(trainX, trainY)\n",
    "    yfit = mod.predict(trainX)\n",
    "    ypredVal = mod.predict(valX)\n",
    "    ypredTest = mod.predict(testX)\n",
    "    \n",
    "    # train result, validation result, test result\n",
    "    return accuracy_score(trainY, yfit), accuracy_score(valY, ypredVal), accuracy_score(testY, ypredTest)\n",
    "    \n",
    "def loop_testing(model, n_test: int, xArr: sp.sparse.csr_matrix, yArr: np.array, testXArr: sp.sparse.csr_matrix, testYArr: np.array) -> dict[str: list([float, float])]:\n",
    "    thedict = {}\n",
    "\n",
    "    i = 0\n",
    "    n = xArr.shape[0]\n",
    "    n_sample = math.ceil(n / n_test)\n",
    "    x_sam, y_sam = np.array([]), np.array([])\n",
    "    while i < n_test:\n",
    "        percent_sample = n_sample/xArr.shape[0]\n",
    "        if percent_sample >= 1:\n",
    "            xmain, ymain = xArr, yArr\n",
    "        else:\n",
    "            xmain, xArr, ymain, yArr = train_test_split(xArr, yArr, train_size=percent_sample, stratify=yArr, random_state=123)\n",
    "        \n",
    "        x_sam = sp.sparse.vstack((x_sam, xmain)) if x_sam.shape[0] != 0 else xmain\n",
    "        y_sam = np.hstack((y_sam, ymain)) if y_sam.shape[0] != 0 else ymain\n",
    "        xtrain, xtest, ytrain, ytest = train_test_split(x_sam, y_sam, train_size=0.9, stratify=y_sam, random_state=123)\n",
    "\n",
    "        resTrain, resVal, resTest = accuracyTrainTest(model, xtrain, ytrain, xtest, ytest, testXArr, testYArr)\n",
    "        thedict[x_sam.shape[0]] = [resTrain, resVal, resTest]\n",
    "\n",
    "        i += 1\n",
    "    return thedict\n",
    "\n",
    "# visualization\n",
    "def show_result(ypred: np.array, ytarget: np.array) -> None:\n",
    "    label = ['Positive', 'Neutral', 'Negative']\n",
    "    report = classification_report(ypred, ytarget, labels=label)\n",
    "    conf_matrix = confusion_matrix(ypred, ytarget, labels=label)\n",
    "\n",
    "    group_names = ['True Pos', 'False Neu', 'False Neg',\n",
    "                   'False Pos', 'True Neu', 'False Neg',\n",
    "                   'False Pos', 'False Neu', 'True Neg']\n",
    "    group_percentages = ['{0:.2%}'.format(value) for value in conf_matrix.flatten() / np.sum(conf_matrix)]\n",
    "    labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(group_names, group_percentages, conf_matrix.flatten())]\n",
    "    labels = np.asarray(labels).reshape(3, 3)\n",
    "\n",
    "    df_cm = pd.DataFrame(data=conf_matrix, index=label, columns=label)\n",
    "\n",
    "    print(report)\n",
    "    fix, axe = plt.subplots(figsize=(15, 8), nrows=1, ncols=1)\n",
    "    sns.heatmap(df_cm, xticklabels=label, yticklabels=label, annot=labels, ax=axe, cmap = 'Blues', fmt = '')\n",
    "    plt.show();\n",
    "\n",
    "def visualized_loopTesting(result_loopTesting: dict[str: list([float, float])]) -> None:\n",
    "    train_res, val_res, test_res = dict(), dict(), dict()\n",
    "    for res in result_loopTesting:\n",
    "        train_res[res] = result[res][0]\n",
    "        val_res[res] = result[res][1]\n",
    "        test_res[res] = result[res][2]\n",
    "\n",
    "    fig, axe = plt.subplots(figsize=(15, 8), nrows=1, ncols=1)\n",
    "    sns.lineplot(y=train_res.values(), x=train_res.keys() , ax=axe, label='train-result')\n",
    "    sns.lineplot(y=val_res.values(), x=val_res.keys(), ax=axe, label='val-result')\n",
    "    sns.lineplot(y=test_res.values(), x=test_res.keys(), ax=axe, label='test-result')\n",
    "    axe.set_title(\"Highest Score: {}\".format(max(test_res.values())))\n",
    "    axe.set_xlabel(\"Dataset Size\")\n",
    "    axe.set_ylabel(\"Accuracy\")\n",
    "    axe.set_ylim([0.65, 1.0])\n",
    "    axe.xaxis.set_major_formatter(FormatStrFormatter('%.0f'))\n",
    "    plt.show();\n",
    "\n",
    "# others\n",
    "def correlation(x: list, y: list) -> float:\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    bar_x = x.mean()\n",
    "    bar_y = y.mean()\n",
    "    sum_xixbar_yiybar = sum((x - bar_x) * (y - bar_y))\n",
    "    sum_xixbar2 = sum((x - bar_x) ** 2)\n",
    "    sum_yiybar2 = sum((y - bar_y) ** 2)\n",
    "\n",
    "    numer = sum_xixbar_yiybar\n",
    "    denom = (sum_xixbar2 * sum_yiybar2) ** (1 / 2)\n",
    "\n",
    "    r = numer / denom if denom != 0 else 0\n",
    "\n",
    "    return r\n",
    "\n",
    "def gradient(x: list, y: list) -> float:\n",
    "    p1 = [x[0], y[0]]\n",
    "    p2 = [x[2], y[2]]\n",
    "\n",
    "    numer = p1[1] - p2[1]\n",
    "    denom = p1[0] - p2[0]\n",
    "\n",
    "    return numer / denom\n",
    "    \n",
    "\n",
    "df = pd.read_json('../data/Amazon_GroceryandGourmetFood.json', lines=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdata = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdata.fillna(\"\", inplace=True)\n",
    "\n",
    "if len(dfdata['overall'].unique()) != 3:\n",
    "    dfdata['overallOri'] = dfdata['overall']\n",
    "dfdata['overall'] = dfdata['overallOri'].apply(lambda x: \"Positive\" if x > 3 else \"Negative\" if x < 3 else \"Neutral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_n = 500 * 1000\n",
    "dfdataMain, _ = train_test_split(dfdata, train_size=target_n / dfdata.shape[0], stratify=dfdata['overall'], random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for training & testing\n",
    "target_n = 450 * 1000\n",
    "dfdataTrain, dfdataTest = train_test_split(dfdataMain, train_size=target_n / dfdataMain.shape[0], stratify=dfdataMain['overall'], random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for validation\n",
    "target_n = 45 * 1000\n",
    "dfdataValid, dfdataTrainSub = train_test_split(dfdataTrain, train_size=target_n / dfdataTrain.shape[0], stratify=dfdataTrain['overall'], random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for validation training & testing\n",
    "target_n = 40 * 1000\n",
    "dfdataValidTrain, dfdataValidTest = train_test_split(dfdataValid, train_size=target_n / dfdataValid.shape[0], stratify=dfdataValid['overall'], random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsampling minority group for validation training\n",
    "if statistics.variance(dfdataValidTrain.groupby(by=['overall'])['overall'].count().values) >= 10 * 1000:\n",
    "    dfdataValidTrain = balancing_upsampling(dfdataValidTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(txt: str) -> str:\n",
    "    txt = pipe_cleaningText(txt)\n",
    "    txt = pipe_lemmatization(txt)\n",
    "    txt = pipe_normalized(txt, 5)\n",
    "    return txt\n",
    "    \n",
    "tdf1 = dfdataValidTrain.copy()\n",
    "tdf2 = dfdataValidTest.copy()\n",
    "tdf1['reviewText'] = tdf1['reviewText'].apply(lambda x: pipeline(x))\n",
    "tdf2['reviewText'] = tdf2['reviewText'].apply(lambda x: pipeline(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "xArr, featureModel = feature_bowUniBi(tdf1['reviewText'], 150000)\n",
    "yArr = tdf1['overall'].to_numpy()\n",
    "testXArr, testYArr = featureModel.transform(tdf2['reviewText']), tdf2['overall'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 4: mlp\n",
    "\n",
    "mlp_model = MLPClassifier()\n",
    "result = loop_testing(mlp_model, 10, xArr, yArr, testXArr, testYArr)\n",
    "print(\"Correlation last 3: {:.4f}\".format(correlation(list(result.keys())[-3:], [x[2] for x in result.values()][-3:])))\n",
    "print(\"Gradient last 3: {:.4f}\".format(gradient(range(1, 4), [x[2] for x in result.values()][-3:])))\n",
    "visualized_loopTesting(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "46bc6b800f911b1c1d0f43a06dbc3139270a12e6ebc6e060d97d05b2a2df1825"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
